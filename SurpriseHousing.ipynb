{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #numerical analysis library\n",
    "import numpy as np #data analysis library\n",
    "import matplotlib.pyplot as plt #Visualization library\n",
    "import seaborn as sns #visualization library\n",
    "from sklearn.linear_model import LinearRegression #linear regression modelling \n",
    "from sklearn.metrics import r2_score, mean_squared_error #accuracy check metric library\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the head of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the various aspects of the data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "#importing dataset through pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=data.isnull().sum() #checking the no. of null enteries in the dataset as part of data cleaning process \n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data.drop(data.columns[[6,57,72,73,74,32,33,35,59,60,63]],axis=1)#dropping all the columns which has aroudn 50% or more missing values and based on domain knowledge, irrelavant on basis on problem & data dictionary \n",
    "n=data1.isnull().sum() #checking the no. of null enteries in the dataset as part of data cleaning process \n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data1.columns:\n",
    "    print(i,data1[i].nunique())\n",
    "#to get the info for uniques values in the individual drivers shortlisted so as to segregate into categorical & continous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LotFrontage=data1['LotFrontage'].mode()[0]#calculation the mode\n",
    "BsmtQual=data1['BsmtQual'].mode()[0]#calculation the mode\n",
    "BsmtCond=data1['BsmtCond'].mode()[0]#calculation the mode\n",
    "Electrical=data1['Electrical'].mode()[0]#calculation the mode\n",
    "GarageType=data1['GarageType'].mode()[0]#calculation the mode\n",
    "GarageCond=data1['GarageCond'].mode()[0]#calculation the mode\n",
    "data1['LotFrontage']=data1['LotFrontage'].replace(np.nan,LotFrontage)#replacing the NaN values with mode as it is a categorical driver\n",
    "data1['BsmtQual']=data1['BsmtQual'].replace(np.nan,BsmtQual)#replacing the NaN values with mode as it is a categorical driver\n",
    "data1['BsmtCond']=data1['Electrical'].replace(np.nan,Electrical)#replacing the NaN values with mode as it is a categorical driver\n",
    "data1['Electrical']=data1['Electrical'].replace(np.nan,Electrical)#replacing the NaN values with mode as it is a categorical driver\n",
    "data1['GarageType']=data1['GarageType'].replace(np.nan,GarageType)#replacing the NaN values with mode as it is a categorical driver\n",
    "data1['GarageCond']=data1['GarageCond'].replace(np.nan,GarageCond)#replacing the NaN values with mode as it is a categorical driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=data1.isnull().sum() #checking the no. of null enteries in the dataset as part of data cleaning process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data1.drop(data1.columns[[0,1,10,11,13,16,17,18,9,7,65,66,64,58,59,60,61,62,54,55,56,57,51,59,38,36,29,31,33,34,26,24,23,22,21]],axis=1)#dropping all the columns which has aroudn 50% or more missing values and based on domain knowledge, irrelavant on basis on problem & data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=data2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MasVnrArea=data2['MasVnrArea'].mode()[0]#calculation the mode of MasVnrArea\n",
    "data2['MasVnrArea']=data2['MasVnrArea'].replace(np.nan,MasVnrArea)#replacing the NaN values of MasVnrArea driver with mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2.describe()\n",
    "#calculating some statistical data like percentile, mean and std of the numerical values of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data2)#pairplot command\n",
    "plt.show()\n",
    "#ploting multiple scatterplot for continous variable to understand there relevance relative to target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (19,9)) #fixing the figure size of the plot\n",
    "sns.heatmap(data2.corr(),annot=True,cmap='Greens',fmt='.1%',cbar=False)#running command for heatmap for correlation of continous variable\n",
    "plt.show()\n",
    "#correlation matrix formed through heatmap to understand the correlation of continous variables & there relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #importing relevant libraries for model building\n",
    "\n",
    "# We specify this so that the train and test data set always have the same rows, respectively\n",
    "np.random.seed(0)\n",
    "df_train, df_test = train_test_split(data2, train_size = 0.7, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing into X and Y sets for the model building\n",
    "y_train = df_train.pop('SalePrice')\n",
    "X_train = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant or intercept & selecting GrLivArea as first feature or driver it has the highest correlation with dependent variable\n",
    "X_train_lm = sm.add_constant(X_train[['GrLivArea']])\n",
    "\n",
    "# Create a first fitted model\n",
    "lr = sm.OLS(y_train, X_train_lm).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the parameters obtained\n",
    "lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the data with a scatter plot and the fitted regression line\n",
    "plt.scatter(X_train_lm.iloc[:, 1], y_train)\n",
    "plt.plot(X_train_lm.iloc[:, 1], 23345.502827 +103.555175*X_train_lm.iloc[:, 1], 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the linear regression model obtained\n",
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning additional feature variables to X\n",
    "X_train_lm = X_train[['GrLivArea','1stFlrSF','FullBath','TotRmsAbvGrd','YearRemodAdd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "X_train_lm = sm.add_constant(X_train_lm)\n",
    "\n",
    "lr = sm.OLS(y_train, X_train_lm).fit()\n",
    "\n",
    "lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary of the model\n",
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the p-values in the above result we have it looks like the variables are really significant\n",
    "And also our adjusted R2 has increase from 48% to 64%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning additional feature variables to X on the basis of the REF & above correlation matrix \n",
    "X_train_lm_1 = X_train[['1stFlrSF','FullBath','YearRemodAdd','MasVnrArea','Fireplaces','BsmtFullBath','BedroomAbvGr','2ndFlrSF','KitchenAbvGr']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg= LinearRegression()\n",
    "reg.fit(X_train_lm_1,y_train)\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear model with new features\n",
    "\n",
    "import statsmodels.api as sm\n",
    "X_train_lm_1 = sm.add_constant(X_train_lm_1)\n",
    "lr1 = sm.OLS(y_train, X_train_lm_1).fit()\n",
    "lr1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary of the model\n",
    "\n",
    "print(lr1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the p-values in the above result we have it looks like of the variables are really significant\n",
    "And also our adjusted R2 has increase from 64% to 72%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor #importing relevant libraries to look for multicollinearity among feature in model lr1\n",
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_lm_1.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_lm_1.values, i) for i in range(X_train_lm_1.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif\n",
    "\n",
    "#As we can see there is no feature in model lr1 that are high corelation among each other by considering VIF<5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the p-values in the above result we have it looks like the variables are really significant apart from working day as it has p value higher than 0.5\n",
    "But our adjusted R2 has stopped improving it means we will stick to our previous model lr1 or feature in X_train_lm_1  that are ['Light Snow or Rain','yr','windspeed','temp','Mist Cloud','spring']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Analysis of the train data\n",
    "\n",
    "So, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pre = lr1.predict(X_train_lm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, y_train_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the error terms\n",
    "fig = plt.figure()\n",
    "sns.distplot((y_train - y_train_pre), bins = 20)\n",
    "fig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \n",
    "plt.xlabel('Errors', fontsize = 18)                         # X-label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above figure our model residual is perfectly aligned at mean=0 which show its an acceptable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "reg = LinearRegression() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regularization Techniquie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Ridge Regression with varying the hyperparameter 'lambda'\n",
    "\n",
    "X_seq = np.linspace(X_train_lm_1.min(),X_train_lm_1.max(),300) # values to be considered for predictor variable\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1] # Higher the value of lambda,more the regularization \n",
    "for i in lambdas: # for each lambda we get different model coefficients\n",
    "    degree = 7 # Degree for polynomial regression\n",
    "    # Creating degree  7 features\n",
    "    ridgecoef = PolynomialFeatures(degree)\n",
    "    # Transforming input features to polynomial features   \n",
    "    X_poly = ridgecoef.fit_transform(X_train_lm_1)\n",
    "    ridgereg = Ridge(alpha = i) # Initialize the Ridge Regression model with a specific lambda\n",
    "    ridgereg.fit(X_poly, y_train) # fit the model on the polynomial features\n",
    "    \n",
    "    #Computing the r2 score\n",
    "    y_pred = ridgereg.predict(ridgecoef.fit_transform(X_train_lm_1))\n",
    "    print(\"r2 score = \" + str(r2_score(y_train, y_pred))) \n",
    "    print(ridgereg.coef_) # model coefficients  \n",
    "                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regularization Techniquie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Lasso Regression with varying the hyperparameter 'lambda'\n",
    "\n",
    "lambdas = [0.001,0.01, 0.1, 1,10]\n",
    "for i in lambdas:\n",
    "    degree = 10\n",
    "    # Creating degree 5 features\n",
    "    lassocoef = PolynomialFeatures(degree)\n",
    "    # Transforming input features to polynomial features \n",
    "    X_poly = lassocoef.fit_transform(X_train_lm_1)\n",
    "    lassoreg = Lasso(alpha = i)\n",
    "    lassoreg.fit(X_poly, y_train)\n",
    "   \n",
    "    # Compute R^2 \n",
    "    y_pred = lassoreg.predict(lassocoef.fit_transform(X_train_lm_1))\n",
    "    print(\"r2 score = \" + str(r2_score(y_train, y_pred)))\n",
    "    print(lassoreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  0.00000000e+00 -1.20669076e-30 ...  7.68504190e-25\n",
      "  2.46803982e-27  7.86796028e-30]\n",
      "r2 score = 0.9136093871569811\n",
      "[ 0.00000000e+00  0.00000000e+00  8.37695324e+00 ... -1.45802563e-06\n",
      " -7.63477913e-04  4.07303288e-01]\n",
      "r2 score = 0.9448383496204721\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "ridgecoef = PolynomialFeatures(7, include_bias = True) \n",
    "# Transforming input features to polynomial features \n",
    "X_poly = ridgecoef.fit_transform(X_train_lm_1)\n",
    "ridgereg = Ridge(alpha = 0.001) # Initialize the Ridge Regression model with a specific lambda\n",
    "ridgereg.fit(X_poly, y_train) # fit the model on the polynomial features\n",
    "print(ridgereg.coef_)    \n",
    "y_pred = ridgereg.predict(ridgecoef.fit_transform(X_train_lm_1))\n",
    "print(\"r2 score = \" + str(r2_score(y_train, y_pred)))\n",
    "\n",
    "# Lasso Regression\n",
    "lassocoef = PolynomialFeatures(9) \n",
    "# Transforming input features to polynomial features \n",
    "X_poly = lassocoef.fit_transform(X_train_lm_1)\n",
    "lassoreg = Lasso(alpha = 0.001)\n",
    "lassoreg.fit(X_poly, y_train)\n",
    "print(lassoreg.coef_)\n",
    "y_pred = lassoreg.predict(lassocoef.fit_transform(X_train_lm_1))\n",
    "print(\"r2 score = \" + str(r2_score(y_train, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6aae9e3b5b1a6982bb75e9b88e9c5453921bcfe9f533234dee128d1b4ee2325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
